


import plotly.express as px
import pandas as pd
import numpy as np
from datetime import timedelta
import matplotlib.pyplot as plt
import seaborn as sns
import os
import folium
from folium.plugins import HeatMap
from folium.plugins import HeatMapWithTime
import datetime
from sklearn.cluster import KMeans
from geopy.distance import geodesic 


# Load the dataset
file_path = 'rome_u_journeys.csv'  # Assuming the file is available in the working directory
# Load the CSV file into a DataFrame with specified column names
try:
    df = pd.read_csv(file_path, header=0)
    # Display the first few rows of the dataset to confirm loading
    df.head()
except FileNotFoundError:
    "The file 'rome_u_journeys.csv' was not found in the specified directory."

df





df.describe(), df.info()


import pandas as pd
import numpy as np

# Filtrar filas que contengan al menos un valor vacío, None, NaN, 0, o 0.0
rows_with_issues = df[
    df.isnull().any(axis=1) | (df == 0).any(axis=1)
]

# Obtener los índices (números de fila) de las filas con problemas
row_numbers = rows_with_issues.index.tolist()

print("Filas con valores vacíos, None, NaN, 0 o 0.0:", row_numbers)




# Distribución de tiempo de recorrido
plt.figure(figsize=(15, 10))
sns.histplot(df['tt'], bins=50, kde=True, color='skyblue')
plt.title('Distribución de Tiempo de Recorrido')
plt.xlabel('Tiempo de Recorrido (s)')
plt.ylabel('Frecuencia')
plt.show()

# Distribución de distancias
plt.figure(figsize=(15, 10))
sns.histplot(df['dis'], bins=50, kde=True, color='orange')
plt.title('Distribución de Distancias')
plt.xlabel('Distancia (m)')
plt.ylabel('Frecuencia')
plt.show()

# Velocidad media
plt.figure(figsize=(15, 10))
sns.boxplot(x=df['vel'], color='green')
plt.title('Distribución de Velocidad')
plt.xlabel('Velocidad (km/h)')
plt.show()


# Convertir la columna `tsO` a formato datetime si no lo has hecho
# Asegurarnos de que la columna `tsO` esté en formato datetime con el formato correcto
df['tsO'] = pd.to_datetime(df['tsO'], format='%d/%m/%Y %H:%M:%S', dayfirst=True)

# **Gráfico 1: Número de viajes por hora**
# Extraer la hora de cada viaje
df['hour'] = df['tsO'].dt.hour

# Agrupar los datos por fecha y hora y contar el número de viajes
viajes_por_hora = df.groupby(df['tsO'].dt.floor('H')).size().reset_index(name='Numero_viajes')

# Renombrar la columna agrupada como 'Fecha_Hora' para claridad
viajes_por_hora.rename(columns={'tsO': 'Fecha_Hora'}, inplace=True)

# Crear el gráfico interactivo
fig1 = px.line(
    viajes_por_hora,
    x='Fecha_Hora',
    y='Numero_viajes',
    title='Número de viajes por hora durante febrero de 2021',
    labels={'Fecha_Hora': 'Fecha y Hora', 'Numero_viajes': 'Número de viajes'},
)

# Mejorar la visualización con detalles en los ejes y el hover
fig1.update_layout(
    xaxis_title='Fecha y Hora',
    yaxis_title='Número de viajes',
    template='plotly',
    hovermode='x'
)

# Mostrar el gráfico interactivo
fig1.show()

# **Gráfico 2: Promedio de viajes por hora del día**
# Calcular el número promedio de viajes por cada hora
average_trips_per_hour = df.groupby('hour').size().groupby(level=0).mean()

# Crear un gráfico de barras interactivo con Plotly
fig2 = px.bar(
    average_trips_per_hour,
    x=average_trips_per_hour.index,
    y=average_trips_per_hour.values,
    labels={'x': 'Hora del día', 'y': 'Número promedio de viajes'},
    title='Número promedio de viajes que comenzaron a cada hora del día en febrero de 2021'
)

# Configurar ejes y diseño para mejor visualización
fig2.update_layout(
    xaxis_title='Hora del día',
    yaxis_title='Número promedio de viajes',
    xaxis=dict(dtick=1),  # Mostrar todas las horas en el eje X
    template='plotly',     # Estilo para hacer el gráfico más agradable
    hovermode="x"          # Para un efecto de hover intuitivo
)

# Mostrar el gráfico interactivo
fig2.show()

# **Gráfico 3: Promedio de viajes por día de la semana**
# Extraer el día de la semana como nombre (lunes, martes, etc.)
df['weekday'] = df['tsO'].dt.day_name()

# Calcular el número promedio de viajes por cada día de la semana
average_trips_per_weekday = df.groupby('weekday').size() / 4  # Dividido entre 4 semanas

# Asegurar el orden correcto de los días
ordered_days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']
average_trips_per_weekday = average_trips_per_weekday.reindex(ordered_days)

# Crear un gráfico de barras interactivo con Plotly
fig3 = px.bar(
    average_trips_per_weekday,
    x=average_trips_per_weekday.index,
    y=average_trips_per_weekday.values,
    labels={'x': 'Día de la semana', 'y': 'Número promedio de viajes'},
    title='Número promedio de viajes que comenzaron cada día de la semana en febrero de 2021'
)

# Configurar ejes y diseño para mejor visualización
fig3.update_layout(
    xaxis_title='Día de la semana',
    yaxis_title='Número promedio de viajes',
    xaxis=dict(categoryorder='array', categoryarray=ordered_days),  # Mantener el orden correcto
    template='plotly',             # Estilo para hacer el gráfico más agradable
    hovermode="x"                  # Para un efecto de hover intuitivo
)

# Mostrar el gráfico interactivo
fig3.show()



# Calcular la velocidad promedio calculada
df['vel_calculada'] = df['dis'] / df['tt']  # Velocidad = Distancia / Tiempo

# Relación entre distancia y precio
fig1 = px.scatter(
    df, x='dis', y='price', 
    title='Relación entre Distancia y Precio',
    labels={'dis': 'Distancia (m)', 'price': 'Precio (€)'},
    opacity=0.5
)
fig1.update_traces(marker=dict(size=6))
fig1.update_layout(template='plotly')

# Relación entre tiempo y precio
fig2 = px.scatter(
    df, x='tt', y='price', 
    title='Relación entre Tiempo y Precio',
    labels={'tt': 'Tiempo de Recorrido (s)', 'price': 'Precio (€)'},
    color_discrete_sequence=['orange'],
    opacity=0.5
)
fig2.update_traces(marker=dict(size=6))
fig2.update_layout(template='plotly')

# Relación entre distancia y tiempo
fig3 = px.scatter(
    df, x='dis', y='tt', 
    title='Relación entre Distancia y Tiempo de Recorrido',
    labels={'dis': 'Distancia (m)', 'tt': 'Tiempo (s)'},
    color_discrete_sequence=['purple'],
    opacity=0.5
)
fig3.update_traces(marker=dict(size=6))
fig3.update_layout(template='plotly')

# Mostrar los gráficos
fig1.show()
fig2.show()
fig3.show()





# Convertir la columna `tsO` a formato datetime
df['tsO'] = pd.to_datetime(df['tsO'], format='%d/%m/%Y %H:%M:%S')

# Extraer la hora de `tsO` para agrupar los datos
df['hour'] = df['tsO'].dt.hour

# Crear un mapa base centrado en la ubicación media de los datos
mapa_base = folium.Map(location=[df['latO'].mean(), df['lonO'].mean()], zoom_start=12)

# Iterar sobre cada hora y agregar un HeatMap a la capa de horas
for hour in range(24):
    # Filtrar el DataFrame para la hora actual
    df_hour = df[df['hour'] == hour]
    
    # Crear una lista de puntos (latitud, longitud) para el HeatMap
    heat_data = list(zip(df_hour['latO'], df_hour['lonO']))
    
    # Agregar un mapa de calor para esta hora
    HeatMap(heat_data, radius=10, blur=15, max_zoom=1, name=f"Hora {hour}:00").add_to(mapa_base)

# Añadir una capa de control para activar/desactivar las capas de cada hora
folium.LayerControl().add_to(mapa_base)

# Guardar el mapa interactivo
mapa_base.save('mapa_calor_por_hora.html')


# Convertir la columna `tsO` a formato datetime
df['tsO'] = pd.to_datetime(df['tsO'], format='%d/%m/%Y %H:%M:%S')

# Extraer la hora de `tsO` para agrupar los datos
df['hour'] = df['tsO'].dt.hour

# Crear un diccionario para almacenar los datos de cada hora
hourly_data = []
for hour in range(24):
    # Filtrar los datos para cada hora
    df_hour = df[df['hour'] == hour]
    
    # Crear una lista de puntos (latitud, longitud, intensidad) para el HeatMap
    heat_data = [[row['latO'], row['lonO'], 1] for index, row in df_hour.iterrows()]
    
    # Añadir los datos de la hora actual al diccionario
    hourly_data.append(heat_data)

# Crear el mapa base centrado en la ubicación media de los datos
mapa_base = folium.Map(location=[df['latO'].mean(), df['lonO'].mean()], zoom_start=12)

# Crear el mapa de calor con el control de tiempo usando colores cálidos
HeatMapWithTime(
    hourly_data,
    radius=10,
    gradient={0.2: 'yellow', 0.5: 'orange', 0.7: 'red', 1: 'darkred'},  # Colores cálidos
    auto_play=False,
    max_opacity=0.8
).add_to(mapa_base)

# Guardar el mapa interactivo
mapa_base.save('mapa_calor_por_hora_interactivo.html')







# Combinar las coordenadas de origen y destino en un solo conjunto de datos
coords = pd.DataFrame({
    'lat': pd.concat([df['latO'], df['latD']], ignore_index=True),
    'lon': pd.concat([df['lonO'], df['lonD']], ignore_index=True)
})

# Configurar el número de clusters (estaciones de carga). Puedes ajustar n_clusters según tus necesidades
n_clusters = 15
kmeans = KMeans(n_clusters=n_clusters, n_init=n_clusters, random_state=0)  # Se agrega n_init=10
coords['cluster'] = kmeans.fit_predict(coords[['lat', 'lon']])

# Obtener los centroides (posiciones ideales para las estaciones de carga)
centroids = kmeans.cluster_centers_

# Convertir los centroides en un DataFrame para visualización
centroids_df = pd.DataFrame(centroids, columns=['lat', 'lon'])

# Crear un mapa interactivo con los datos y los centroides
fig = px.scatter_mapbox(
    coords, lat="lat", lon="lon", color="cluster",
    title="Clustering de ubicaciones para estaciones de carga",
    zoom=10, height=600
)

# Agregar los centroides al mapa
fig.add_scattermapbox(
    lat=centroids_df['lat'],
    lon=centroids_df['lon'],
    mode='markers+text',
    marker=dict(size=12, color='red'),
    text=[f'Estación {i+1}' for i in range(len(centroids_df))],
    name="Centroides"
)

# Configuración de estilo de mapa
fig.update_layout(
    mapbox_style="open-street-map",
    mapbox_center={"lat": coords['lat'].mean(), "lon": coords['lon'].mean()},
)

# Mostrar el mapa interactivo
fig.show()


# Combinar las coordenadas de origen y destino en un solo conjunto de datos
coords = pd.DataFrame({
    'lat': pd.concat([df['latO'], df['latD']], ignore_index=True),
    'lon': pd.concat([df['lonO'], df['lonD']], ignore_index=True),
    'tipo': ['origen'] * len(df) + ['destino'] * len(df)
})

# Configurar el número de clusters (estaciones de carga)
n_clusters = 25
kmeans = KMeans(n_clusters=n_clusters, n_init=n_clusters, random_state=0)
coords['cluster'] = kmeans.fit_predict(coords[['lat', 'lon']])

# Obtener los centroides (posiciones ideales para las estaciones de carga)
centroids = kmeans.cluster_centers_
centroids_df = pd.DataFrame(centroids, columns=['lat', 'lon'])

# Función para calcular la distancia de Haversine en kilómetros
def haversine_distance(coord1, coord2):
    return geodesic(coord1, coord2).kilometers

# Crear un DataFrame para almacenar las métricas de cada clúster
metrics = []

# Calcular las métricas para cada clúster
for cluster in range(n_clusters):
    # Filtrar puntos en el clúster actual
    cluster_points = coords[coords['cluster'] == cluster]
    n_points = len(cluster_points)
    
    # Porcentaje de puntos de origen y destino
    n_origen = len(cluster_points[cluster_points['tipo'] == 'origen'])
    n_destino = n_points - n_origen
    pct_origen = n_origen / n_points * 100
    pct_destino = n_destino / n_points * 100
    
    # Coordenadas del centroide
    centroide = centroids[cluster]
    
    # Calcular distancias al centroide
    distances = cluster_points.apply(
        lambda row: haversine_distance((row['lat'], row['lon']), (centroide[0], centroide[1])), axis=1
    )
    avg_distance = distances.mean()  # Distancia media al centroide
    max_distance = distances.max()   # Distancia máxima al centroide
    
    # Guardar métricas en el DataFrame
    metrics.append({
        'Num_Puntos': n_points,
        'Pct_Origen': pct_origen,
        'Pct_Destino': pct_destino,
        'Distancia_Media_km': avg_distance,
        'Distancia_Max_km': max_distance
    })

# Convertir la lista de métricas a un DataFrame
metrics_df = pd.DataFrame(metrics)

# Mostrar el DataFrame con las métricas
print(metrics_df)


import pandas as pd
from sklearn.cluster import KMeans
from geopy.distance import geodesic
import folium
from IPython.display import display
import numpy as np
import warnings

# Ocultar los warnings
warnings.filterwarnings("ignore")

# Combinar las coordenadas de origen y destino en un solo conjunto de datos
coords = pd.DataFrame({
    'lat': pd.concat([df['latO'], df['latD']], ignore_index=True),
    'lon': pd.concat([df['lonO'], df['lonD']], ignore_index=True),
    'tipo': ['origen'] * len(df) + ['destino'] * len(df)
})

# Configurar el número inicial de clusters
n_clusters = 25
kmeans = KMeans(n_clusters=n_clusters, n_init=n_clusters, random_state=0)
coords['cluster'] = kmeans.fit_predict(coords[['lat', 'lon']])

# Obtener los centroides iniciales
centroids = kmeans.cluster_centers_
centroids_df = pd.DataFrame(centroids, columns=['lat', 'lon'])

# Función para calcular la distancia de Haversine en kilómetros
def haversine_distance(coord1, coord2):
    return geodesic(coord1, coord2).kilometers

# Crear un DataFrame para almacenar las métricas de cada clúster
metrics = []
coords_final = pd.DataFrame()
centroids_final = []

# Calcular las métricas para cada clúster
cluster_counter = 0  # contador para identificar los clusters finales
for cluster in range(n_clusters):
    # Filtrar puntos en el clúster actual
    cluster_points = coords[coords['cluster'] == cluster].copy()  # Hacer una copia para evitar el warning
    n_points = len(cluster_points)
    
    # Coordenadas del centroide
    centroide = centroids[cluster]
    
    # Calcular distancias al centroide
    distances = cluster_points.apply(
        lambda row: haversine_distance((row['lat'], row['lon']), (centroide[0], centroide[1])), axis=1
    )
    
    # Calcular el rango intercuartílico (IQR)
    q1 = distances.quantile(0.25)
    q3 = distances.quantile(0.75)
    iqr = q3 - q1
    lower_bound = q1 - 1.5 * iqr
    upper_bound = q3 + 1.5 * iqr

    # Filtrar distancias para excluir los puntos más lejanos
    filtered_distances = distances[(distances >= lower_bound) & (distances <= upper_bound)]
    
    # Calcular la distancia media sin los puntos extremos más alejados
    avg_distance = filtered_distances.mean()

    # Si la distancia media es mayor a 0.5 km (500 m), dividir el cluster
    if avg_distance > 0.5:
        # Hacer k-means con 2 clusters para los puntos de este clúster
        sub_kmeans = KMeans(n_clusters=2, n_init=2, random_state=0)
        cluster_points['sub_cluster'] = sub_kmeans.fit_predict(cluster_points[['lat', 'lon']])
        
        # Obtener los nuevos centroides
        sub_centroids = sub_kmeans.cluster_centers_

        # Calcular métricas para los nuevos sub-clusters
        for sub_cluster in range(2):
            sub_cluster_points = cluster_points[cluster_points['sub_cluster'] == sub_cluster].copy()  # Hacer copia para evitar el warning
            n_sub_points = len(sub_cluster_points)

            sub_centroide = sub_centroids[sub_cluster]

            # Calcular distancias al nuevo centroide
            sub_distances = sub_cluster_points.apply(
                lambda row: haversine_distance((row['lat'], row['lon']), (sub_centroide[0], sub_centroide[1])), axis=1
            )

            # Calcular el rango intercuartílico (IQR) para los subclusters
            q1 = sub_distances.quantile(0.25)
            q3 = sub_distances.quantile(0.75)
            iqr = q3 - q1
            lower_bound = q1 - 1.5 * iqr
            upper_bound = q3 + 1.5 * iqr

            # Filtrar distancias para excluir los puntos más lejanos
            filtered_sub_distances = sub_distances[(sub_distances >= lower_bound) & (sub_distances <= upper_bound)]

            sub_avg_distance = filtered_sub_distances.mean()
            sub_max_distance = sub_distances.max()

            # Guardar métricas de sub-cluster
            metrics.append({
                'Cluster': f"{cluster}.{sub_cluster}",
                'Num_Puntos': n_sub_points,
                'Distancia_Media_km': sub_avg_distance,
                'Distancia_Max_km': sub_max_distance
            })

            # Asignar un identificador de cluster único
            sub_cluster_points['cluster_final'] = cluster_counter
            cluster_counter += 1

            # Agregar puntos a los resultados finales
            coords_final = pd.concat([coords_final, sub_cluster_points], ignore_index=True)

            # Guardar el centroide final
            centroids_final.append(sub_centroide)
    else:
        # Agregar puntos a los resultados finales
        cluster_points['cluster_final'] = cluster_counter
        cluster_counter += 1

        coords_final = pd.concat([coords_final, cluster_points], ignore_index=True)

        # Guardar el centroide final
        centroids_final.append(centroide)

        # Guardar las métricas del cluster sin subdividir
        metrics.append({
            'Cluster': cluster,
            'Num_Puntos': n_points,
            'Distancia_Media_km': avg_distance,
            'Distancia_Max_km': distances.max()
        })

# Convertir la lista de métricas a un DataFrame
metrics_df = pd.DataFrame(metrics)

# Mostrar el DataFrame con las métricas finales
print(metrics_df)


# Convertir la lista de centroides finales a un DataFrame
centroids_final_df = pd.DataFrame(centroids_final, columns=['lat', 'lon'])

# Crear un mapa con Folium
m = folium.Map(location=[coords['lat'].mean(), coords['lon'].mean()], zoom_start=12)

# Generar una lista de colores únicos para cada cluster final
colors = [
    "#%06x" % np.random.randint(0, 0xFFFFFF) for _ in range(cluster_counter)
]

# Agregar todos los puntos al mapa con diferentes colores por cluster
for _, row in coords_final.iterrows():
    cluster_color = colors[int(row['cluster_final'])]
    folium.CircleMarker(location=(row['lat'], row['lon']),
                        radius=3,
                        color=cluster_color,
                        fill=True,
                        fill_color=cluster_color,
                        fill_opacity=0.5).add_to(m)

# Agregar los centroides finales al mapa con un color distinto (rojo)
for _, row in centroids_final_df.iterrows():
    folium.Marker(location=(row['lat'], row['lon']),
                  icon=folium.Icon(color='red', icon='info-sign')).add_to(m)

# Mostrar el mapa inline en Jupyter Notebook
display(m)





# Cargar el archivo XLSX en un DataFrame llamado energia_df
energia_df = pd.read_excel('20210201_20210228_PUN.xlsx') 
energia_df, energia_df.info()


# Asegurarse de que todos los valores en la columna `€/MWh` sean cadenas antes de aplicar operaciones de cadena
energia_df['€/MWh'] = energia_df['€/MWh'].astype(str).str.replace(',', '.')

# Convertir la columna `€/MWh` a numérica
energia_df['€/MWh'] = pd.to_numeric(energia_df['€/MWh'], errors='coerce')

# Convertir la columna `Data` a formato datetime con el formato adecuado
energia_df['Data'] = pd.to_datetime(energia_df['Data'], format='%d/%m/%Y')

# Crear una columna de fecha y hora combinando `Data` y `Ora`
energia_df['Fecha_Hora'] = energia_df['Data'] + pd.to_timedelta(energia_df['Ora'], unit='h')

# Crear el gráfico interactivo
fig = px.line(
    energia_df,
    x='Fecha_Hora',
    y='€/MWh',
    title='Evolución del coste de la energía en Roma durante febrero de 2021',
    labels={'Fecha_Hora': 'Fecha y Hora', '€/MWh': 'Coste en €/MWh'}
)

# Mejorar la visualización con detalles en los ejes y el hover
fig.update_layout(
    xaxis_title='Fecha y Hora',
    yaxis_title='Coste en €/MWh',
    template='plotly',
    hovermode='x'
)

# Mostrar el gráfico interactivo
fig.show()



# Paso 1: Asegurarse de que la columna '€/MWh' sea numérica
energia_df['€/MWh'] = pd.to_numeric(energia_df['€/MWh'], errors='coerce')

# Paso 2: Agrupar los datos por hora y calcular el precio promedio por hora
precio_promedio_por_hora = energia_df.groupby('Ora')['€/MWh'].mean()

# Paso 3: Visualizar los resultados
plt.figure(figsize=(10,6))
plt.plot(precio_promedio_por_hora.index, precio_promedio_por_hora.values, marker='o', color='b')
plt.title('Variación del precio promedio de la electricidad por hora en febrero de 2021')
plt.xlabel('Hora del día')
plt.ylabel('Precio promedio (€ / MWh)')
plt.xticks(range(1, 25))  # Aseguramos que las horas vayan de 1 a 24
plt.grid(True)

# Paso 4: Identificar las horas con los precios más altos y más bajos
hora_max_precio = precio_promedio_por_hora.idxmax()
hora_min_precio = precio_promedio_por_hora.idxmin()
max_precio = precio_promedio_por_hora.max()
min_precio = precio_promedio_por_hora.min()

# Mostrar las horas de los precios más altos y más bajos
print(f"Hora con el precio más alto: {hora_max_precio} h, Precio: €{max_precio:.2f}")
print(f"Hora con el precio más bajo: {hora_min_precio} h, Precio: €{min_precio:.2f}")

# Paso 5: Mostrar el gráfico
plt.show()



