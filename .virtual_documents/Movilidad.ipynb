


import plotly.express as px
import pandas as pd
import numpy as np
from datetime import timedelta
import matplotlib.pyplot as plt
import seaborn as sns
import os
import folium
from folium.plugins import HeatMap
from folium.plugins import HeatMapWithTime
import datetime
from sklearn.cluster import KMeans
from geopy.distance import geodesic 


# Load the dataset
file_path = 'rome_u_journeys.csv'  # Assuming the file is available in the working directory
# Load the CSV file into a DataFrame with specified column names
try:
    df = pd.read_csv(file_path, header=0)
    # Display the first few rows of the dataset to confirm loading
    df.head()
except FileNotFoundError:
    "The file 'rome_u_journeys.csv' was not found in the specified directory."

df


df.describe(), df.info()


import pandas as pd
import numpy as np

# Filtrar filas que contengan al menos un valor vacío, None, NaN, 0, o 0.0
rows_with_issues = df[
    df.isnull().any(axis=1) | (df == 0).any(axis=1)
]

# Obtener los índices (números de fila) de las filas con problemas
row_numbers = rows_with_issues.index.tolist()

print("Filas con valores vacíos, None, NaN, 0 o 0.0:", row_numbers)




# Cálculo del IQR para cada columna de coordenadas (latitud y longitud de origen y destino)
Q1_latO, Q3_latO = df['latO'].quantile(0.25), df['latO'].quantile(0.75)
Q1_lonO, Q3_lonO = df['lonO'].quantile(0.25), df['lonO'].quantile(0.75)
IQR_latO, IQR_lonO = Q3_latO - Q1_latO, Q3_lonO - Q1_lonO

Q1_latD, Q3_latD = df['latD'].quantile(0.25), df['latD'].quantile(0.75)
Q1_lonD, Q3_lonD = df['lonD'].quantile(0.25), df['lonD'].quantile(0.75)
IQR_latD, IQR_lonD = Q3_latD - Q1_latD, Q3_lonD - Q1_lonD

# Definir el umbral para considerar outliers (1.5 veces el IQR es comúnmente usado)
umbral_iqr = 1.5

# Filtrar los outliers en base al IQR y crear un nuevo DataFrame sin ellos
df_filtrado = df[
    (df['latO'] >= Q1_latO - umbral_iqr * IQR_latO) & (df['latO'] <= Q3_latO + umbral_iqr * IQR_latO) &
    (df['lonO'] >= Q1_lonO - umbral_iqr * IQR_lonO) & (df['lonO'] <= Q3_lonO + umbral_iqr * IQR_lonO) &
    (df['latD'] >= Q1_latD - umbral_iqr * IQR_latD) & (df['latD'] <= Q3_latD + umbral_iqr * IQR_latD) &
    (df['lonD'] >= Q1_lonD - umbral_iqr * IQR_lonD) & (df['lonD'] <= Q3_lonD + umbral_iqr * IQR_lonD)
]

# Calcular el número de filas eliminadas
filas_eliminadas = len(df) - len(df_filtrado)

# Mostrar el resumen
filas_eliminadas, len(df_filtrado)






# Convertir la columna `tsO` a formato datetime
df['tsO'] = pd.to_datetime(df['tsO'], format='%d/%m/%Y %H:%M:%S')

# Extraer la hora de `tsO` para agrupar los datos
df['hour'] = df['tsO'].dt.hour

# Crear un mapa base centrado en la ubicación media de los datos
mapa_base = folium.Map(location=[df['latO'].mean(), df['lonO'].mean()], zoom_start=12)

# Iterar sobre cada hora y agregar un HeatMap a la capa de horas
for hour in range(24):
    # Filtrar el DataFrame para la hora actual
    df_hour = df[df['hour'] == hour]
    
    # Crear una lista de puntos (latitud, longitud) para el HeatMap
    heat_data = list(zip(df_hour['latO'], df_hour['lonO']))
    
    # Agregar un mapa de calor para esta hora
    HeatMap(heat_data, radius=10, blur=15, max_zoom=1, name=f"Hora {hour}:00").add_to(mapa_base)

# Añadir una capa de control para activar/desactivar las capas de cada hora
folium.LayerControl().add_to(mapa_base)

# Guardar el mapa interactivo
mapa_base.save('mapa_calor_por_hora.html')


# Convertir la columna `tsO` a formato datetime
df['tsO'] = pd.to_datetime(df['tsO'], format='%d/%m/%Y %H:%M:%S')

# Extraer la hora de `tsO` para agrupar los datos
df['hour'] = df['tsO'].dt.hour

# Crear un diccionario para almacenar los datos de cada hora
hourly_data = []
for hour in range(24):
    # Filtrar los datos para cada hora
    df_hour = df[df['hour'] == hour]
    
    # Crear una lista de puntos (latitud, longitud, intensidad) para el HeatMap
    heat_data = [[row['latO'], row['lonO'], 1] for index, row in df_hour.iterrows()]
    
    # Añadir los datos de la hora actual al diccionario
    hourly_data.append(heat_data)

# Crear el mapa base centrado en la ubicación media de los datos
mapa_base = folium.Map(location=[df['latO'].mean(), df['lonO'].mean()], zoom_start=12)

# Crear el mapa de calor con el control de tiempo usando colores cálidos
HeatMapWithTime(
    hourly_data,
    radius=10,
    gradient={0.2: 'yellow', 0.5: 'orange', 0.7: 'red', 1: 'darkred'},  # Colores cálidos
    auto_play=False,
    max_opacity=0.8
).add_to(mapa_base)

# Guardar el mapa interactivo
mapa_base.save('mapa_calor_por_hora_interactivo.html')







# Combinar las coordenadas de origen y destino en un solo conjunto de datos
coords = pd.DataFrame({
    'lat': pd.concat([df['latO'], df['latD']], ignore_index=True),
    'lon': pd.concat([df['lonO'], df['lonD']], ignore_index=True)
})

# Configurar el número de clusters (estaciones de carga). Puedes ajustar n_clusters según tus necesidades
n_clusters = 15
kmeans = KMeans(n_clusters=n_clusters, n_init=n_clusters, random_state=0)  # Se agrega n_init=10
coords['cluster'] = kmeans.fit_predict(coords[['lat', 'lon']])

# Obtener los centroides (posiciones ideales para las estaciones de carga)
centroids = kmeans.cluster_centers_

# Convertir los centroides en un DataFrame para visualización
centroids_df = pd.DataFrame(centroids, columns=['lat', 'lon'])

# Crear un mapa interactivo con los datos y los centroides
fig = px.scatter_mapbox(
    coords, lat="lat", lon="lon", color="cluster",
    title="Clustering de ubicaciones para estaciones de carga",
    zoom=10, height=600
)

# Agregar los centroides al mapa
fig.add_scattermapbox(
    lat=centroids_df['lat'],
    lon=centroids_df['lon'],
    mode='markers+text',
    marker=dict(size=12, color='red'),
    text=[f'Estación {i+1}' for i in range(len(centroids_df))],
    name="Centroides"
)

# Configuración de estilo de mapa
fig.update_layout(
    mapbox_style="open-street-map",
    mapbox_center={"lat": coords['lat'].mean(), "lon": coords['lon'].mean()},
)

# Mostrar el mapa interactivo
fig.show()


# Combinar las coordenadas de origen y destino en un solo conjunto de datos
coords = pd.DataFrame({
    'lat': pd.concat([df['latO'], df['latD']], ignore_index=True),
    'lon': pd.concat([df['lonO'], df['lonD']], ignore_index=True),
    'tipo': ['origen'] * len(df) + ['destino'] * len(df)
})

# Configurar el número de clusters (estaciones de carga)
n_clusters = 25
kmeans = KMeans(n_clusters=n_clusters, n_init=n_clusters, random_state=0)
coords['cluster'] = kmeans.fit_predict(coords[['lat', 'lon']])

# Obtener los centroides (posiciones ideales para las estaciones de carga)
centroids = kmeans.cluster_centers_
centroids_df = pd.DataFrame(centroids, columns=['lat', 'lon'])

# Función para calcular la distancia de Haversine en kilómetros
def haversine_distance(coord1, coord2):
    return geodesic(coord1, coord2).kilometers

# Crear un DataFrame para almacenar las métricas de cada clúster
metrics = []

# Calcular las métricas para cada clúster
for cluster in range(n_clusters):
    # Filtrar puntos en el clúster actual
    cluster_points = coords[coords['cluster'] == cluster]
    n_points = len(cluster_points)
    
    # Porcentaje de puntos de origen y destino
    n_origen = len(cluster_points[cluster_points['tipo'] == 'origen'])
    n_destino = n_points - n_origen
    pct_origen = n_origen / n_points * 100
    pct_destino = n_destino / n_points * 100
    
    # Coordenadas del centroide
    centroide = centroids[cluster]
    
    # Calcular distancias al centroide
    distances = cluster_points.apply(
        lambda row: haversine_distance((row['lat'], row['lon']), (centroide[0], centroide[1])), axis=1
    )
    avg_distance = distances.mean()  # Distancia media al centroide
    max_distance = distances.max()   # Distancia máxima al centroide
    
    # Guardar métricas en el DataFrame
    metrics.append({
        'Num_Puntos': n_points,
        'Pct_Origen': pct_origen,
        'Pct_Destino': pct_destino,
        'Distancia_Media_km': avg_distance,
        'Distancia_Max_km': max_distance
    })

# Convertir la lista de métricas a un DataFrame
metrics_df = pd.DataFrame(metrics)

# Mostrar el DataFrame con las métricas
print(metrics_df)


import pandas as pd
from sklearn.cluster import KMeans
from geopy.distance import geodesic
import folium
from IPython.display import display
import numpy as np
import warnings

# Ocultar los warnings
warnings.filterwarnings("ignore")

# Combinar las coordenadas de origen y destino en un solo conjunto de datos
coords = pd.DataFrame({
    'lat': pd.concat([df['latO'], df['latD']], ignore_index=True),
    'lon': pd.concat([df['lonO'], df['lonD']], ignore_index=True),
    'tipo': ['origen'] * len(df) + ['destino'] * len(df)
})

# Configurar el número inicial de clusters
n_clusters = 25
kmeans = KMeans(n_clusters=n_clusters, n_init=n_clusters, random_state=0)
coords['cluster'] = kmeans.fit_predict(coords[['lat', 'lon']])

# Obtener los centroides iniciales
centroids = kmeans.cluster_centers_
centroids_df = pd.DataFrame(centroids, columns=['lat', 'lon'])

# Función para calcular la distancia de Haversine en kilómetros
def haversine_distance(coord1, coord2):
    return geodesic(coord1, coord2).kilometers

# Crear un DataFrame para almacenar las métricas de cada clúster
metrics = []
coords_final = pd.DataFrame()
centroids_final = []

# Calcular las métricas para cada clúster
cluster_counter = 0  # contador para identificar los clusters finales
for cluster in range(n_clusters):
    # Filtrar puntos en el clúster actual
    cluster_points = coords[coords['cluster'] == cluster].copy()  # Hacer una copia para evitar el warning
    n_points = len(cluster_points)
    
    # Coordenadas del centroide
    centroide = centroids[cluster]
    
    # Calcular distancias al centroide
    distances = cluster_points.apply(
        lambda row: haversine_distance((row['lat'], row['lon']), (centroide[0], centroide[1])), axis=1
    )
    
    # Calcular el rango intercuartílico (IQR)
    q1 = distances.quantile(0.25)
    q3 = distances.quantile(0.75)
    iqr = q3 - q1
    lower_bound = q1 - 1.5 * iqr
    upper_bound = q3 + 1.5 * iqr

    # Filtrar distancias para excluir los puntos más lejanos
    filtered_distances = distances[(distances >= lower_bound) & (distances <= upper_bound)]
    
    # Calcular la distancia media sin los puntos extremos más alejados
    avg_distance = filtered_distances.mean()

    # Si la distancia media es mayor a 0.5 km (500 m), dividir el cluster
    if avg_distance > 0.5:
        # Hacer k-means con 2 clusters para los puntos de este clúster
        sub_kmeans = KMeans(n_clusters=2, n_init=2, random_state=0)
        cluster_points['sub_cluster'] = sub_kmeans.fit_predict(cluster_points[['lat', 'lon']])
        
        # Obtener los nuevos centroides
        sub_centroids = sub_kmeans.cluster_centers_

        # Calcular métricas para los nuevos sub-clusters
        for sub_cluster in range(2):
            sub_cluster_points = cluster_points[cluster_points['sub_cluster'] == sub_cluster].copy()  # Hacer copia para evitar el warning
            n_sub_points = len(sub_cluster_points)

            sub_centroide = sub_centroids[sub_cluster]

            # Calcular distancias al nuevo centroide
            sub_distances = sub_cluster_points.apply(
                lambda row: haversine_distance((row['lat'], row['lon']), (sub_centroide[0], sub_centroide[1])), axis=1
            )

            # Calcular el rango intercuartílico (IQR) para los subclusters
            q1 = sub_distances.quantile(0.25)
            q3 = sub_distances.quantile(0.75)
            iqr = q3 - q1
            lower_bound = q1 - 1.5 * iqr
            upper_bound = q3 + 1.5 * iqr

            # Filtrar distancias para excluir los puntos más lejanos
            filtered_sub_distances = sub_distances[(sub_distances >= lower_bound) & (sub_distances <= upper_bound)]

            sub_avg_distance = filtered_sub_distances.mean()
            sub_max_distance = sub_distances.max()

            # Guardar métricas de sub-cluster
            metrics.append({
                'Cluster': f"{cluster}.{sub_cluster}",
                'Num_Puntos': n_sub_points,
                'Distancia_Media_km': sub_avg_distance,
                'Distancia_Max_km': sub_max_distance
            })

            # Asignar un identificador de cluster único
            sub_cluster_points['cluster_final'] = cluster_counter
            cluster_counter += 1

            # Agregar puntos a los resultados finales
            coords_final = pd.concat([coords_final, sub_cluster_points], ignore_index=True)

            # Guardar el centroide final
            centroids_final.append(sub_centroide)
    else:
        # Agregar puntos a los resultados finales
        cluster_points['cluster_final'] = cluster_counter
        cluster_counter += 1

        coords_final = pd.concat([coords_final, cluster_points], ignore_index=True)

        # Guardar el centroide final
        centroids_final.append(centroide)

        # Guardar las métricas del cluster sin subdividir
        metrics.append({
            'Cluster': cluster,
            'Num_Puntos': n_points,
            'Distancia_Media_km': avg_distance,
            'Distancia_Max_km': distances.max()
        })

# Convertir la lista de métricas a un DataFrame
metrics_df = pd.DataFrame(metrics)

# Mostrar el DataFrame con las métricas finales
print(metrics_df)


# Convertir la lista de centroides finales a un DataFrame
centroids_final_df = pd.DataFrame(centroids_final, columns=['lat', 'lon'])

# Crear un mapa con Folium
m = folium.Map(location=[coords['lat'].mean(), coords['lon'].mean()], zoom_start=12)

# Generar una lista de colores únicos para cada cluster final
colors = [
    "#%06x" % np.random.randint(0, 0xFFFFFF) for _ in range(cluster_counter)
]

# Agregar todos los puntos al mapa con diferentes colores por cluster
for _, row in coords_final.iterrows():
    cluster_color = colors[int(row['cluster_final'])]
    folium.CircleMarker(location=(row['lat'], row['lon']),
                        radius=3,
                        color=cluster_color,
                        fill=True,
                        fill_color=cluster_color,
                        fill_opacity=0.5).add_to(m)

# Agregar los centroides finales al mapa con un color distinto (rojo)
for _, row in centroids_final_df.iterrows():
    folium.Marker(location=(row['lat'], row['lon']),
                  icon=folium.Icon(color='red', icon='info-sign')).add_to(m)

# Mostrar el mapa inline en Jupyter Notebook
display(m)





# Convertir la columna `tsO` a formato datetime si no lo has hecho
df['tsO'] = pd.to_datetime(df['tsO'], format='%d/%m/%Y %H:%M:%S')

# Extraer la hora de cada viaje
df['hour'] = df['tsO'].dt.hour

# Calcular el número promedio de viajes por cada hora
average_trips_per_hour = df.groupby('hour').size().groupby(level=0).mean()

# Crear un gráfico de barras interactivo con Plotly
fig = px.bar(
    average_trips_per_hour,
    x=average_trips_per_hour.index,
    y=average_trips_per_hour.values,
    labels={'x': 'Hora del día', 'y': 'Número promedio de viajes'},
    title='Número promedio de viajes que comenzaron a cada hora del día en febrero de 2021'
)

# Configurar ejes y diseño para mejor visualización
fig.update_layout(
    xaxis_title='Hora del día',
    yaxis_title='Número promedio de viajes',
    xaxis=dict(dtick=1),  # Mostrar todas las horas en el eje X
    template='plotly',     # Estilo para hacer el gráfico más agradable
    hovermode="x"          # Para un efecto de hover intuitivo
)

# Mostrar el gráfico interactivo
fig.show()


# Asegurarse de que la columna `tsO` esté en formato datetime
df['tsO'] = pd.to_datetime(df['tsO'], format='%d/%m/%Y %H:%M:%S')

# Agrupar los datos por fecha y hora y contar el número de viajes
viajes_por_hora = df.groupby(df['tsO'].dt.floor('H')).size().reset_index(name='Numero_viajes')

# Renombrar la columna agrupada como 'Fecha_Hora' para claridad
viajes_por_hora.rename(columns={'tsO': 'Fecha_Hora'}, inplace=True)

# Crear el gráfico interactivo
fig = px.line(
    viajes_por_hora,
    x='Fecha_Hora',
    y='Numero_viajes',
    title='Número de viajes por hora durante febrero de 2021',
    labels={'Fecha_Hora': 'Fecha y Hora', 'Numero_viajes': 'Número de viajes'},
)

# Mejorar la visualización con detalles en los ejes y el hover
fig.update_layout(
    xaxis_title='Fecha y Hora',
    yaxis_title='Número de viajes',
    template='plotly',
    hovermode='x'
)

# Mostrar el gráfico interactivo
fig.show()





# Cargar el archivo XLSX en un DataFrame llamado energia_df
energia_df = pd.read_excel('20210201_20210228_PUN.xlsx') 
energia_df, energia_df.info()


# Asegurarse de que todos los valores en la columna `€/MWh` sean cadenas antes de aplicar operaciones de cadena
energia_df['€/MWh'] = energia_df['€/MWh'].astype(str).str.replace(',', '.')

# Convertir la columna `€/MWh` a numérica
energia_df['€/MWh'] = pd.to_numeric(energia_df['€/MWh'], errors='coerce')

# Convertir la columna `Data` a formato datetime con el formato adecuado
energia_df['Data'] = pd.to_datetime(energia_df['Data'], format='%d/%m/%Y')

# Crear una columna de fecha y hora combinando `Data` y `Ora`
energia_df['Fecha_Hora'] = energia_df['Data'] + pd.to_timedelta(energia_df['Ora'], unit='h')

# Crear el gráfico interactivo
fig = px.line(
    energia_df,
    x='Fecha_Hora',
    y='€/MWh',
    title='Evolución del coste de la energía en Roma durante febrero de 2021',
    labels={'Fecha_Hora': 'Fecha y Hora', '€/MWh': 'Coste en €/MWh'}
)

# Mejorar la visualización con detalles en los ejes y el hover
fig.update_layout(
    xaxis_title='Fecha y Hora',
    yaxis_title='Coste en €/MWh',
    template='plotly',
    hovermode='x'
)

# Mostrar el gráfico interactivo
fig.show()




