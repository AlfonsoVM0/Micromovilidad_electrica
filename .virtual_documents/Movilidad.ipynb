


import plotly.express as px
import pandas as pd
import numpy as np
from datetime import timedelta, datetime
from ipywidgets import interact, widgets
import matplotlib.pyplot as plt
import matplotlib.colors as mcolors
import matplotlib.cm as cm
import seaborn as sns
import os
import folium
from folium.plugins import HeatMap
from folium.plugins import HeatMapWithTime
import datetime
from sklearn.cluster import KMeans
from geopy.distance import geodesic 
import warnings


# Load the dataset
file_path = 'rome_u_journeys.csv'  # Assuming the file is available in the working directory
# Load the CSV file into a DataFrame with specified column names
try:
    df = pd.read_csv(file_path, header=0)
    # Display the first few rows of the dataset to confirm loading
    df.head()
except FileNotFoundError:
    "The file 'rome_u_journeys.csv' was not found in the specified directory."

df





df.describe(), df.info()


import pandas as pd
import numpy as np

# Filtrar filas que contengan al menos un valor vacío, None, NaN, 0, o 0.0
rows_with_issues = df[
    df.isnull().any(axis=1) | (df == 0).any(axis=1)
]

# Obtener los índices (números de fila) de las filas con problemas
row_numbers = rows_with_issues.index.tolist()

print("Filas con valores vacíos, None, NaN, 0 o 0.0:", row_numbers)




# Distribución de tiempo de recorrido
plt.figure(figsize=(15, 10))
sns.histplot(df['tt'], bins=50, kde=True, color='skyblue')
plt.title('Distribución de Tiempo de Recorrido')
plt.xlabel('Tiempo de Recorrido (s)')
plt.ylabel('Frecuencia')
plt.show()

# Distribución de distancias
plt.figure(figsize=(15, 10))
sns.histplot(df['dis'], bins=50, kde=True, color='orange')
plt.title('Distribución de Distancias')
plt.xlabel('Distancia (m)')
plt.ylabel('Frecuencia')
plt.show()

# Velocidad media
plt.figure(figsize=(15, 10))
sns.boxplot(x=df['vel'], color='green')
plt.title('Distribución de Velocidad')
plt.xlabel('Velocidad (km/h)')
plt.show()


# Convertir la columna `tsO` a formato datetime si no lo has hecho
# Asegurarnos de que la columna `tsO` esté en formato datetime con el formato correcto
df['tsO'] = pd.to_datetime(df['tsO'], format='%d/%m/%Y %H:%M:%S', dayfirst=True)

# **Gráfico 1: Número de viajes por hora**
# Extraer la hora de cada viaje
df['hour'] = df['tsO'].dt.hour

# Agrupar los datos por fecha y hora y contar el número de viajes
viajes_por_hora = df.groupby(df['tsO'].dt.floor('H')).size().reset_index(name='Numero_viajes')

# Renombrar la columna agrupada como 'Fecha_Hora' para claridad
viajes_por_hora.rename(columns={'tsO': 'Fecha_Hora'}, inplace=True)

# Crear el gráfico interactivo
fig1 = px.line(
    viajes_por_hora,
    x='Fecha_Hora',
    y='Numero_viajes',
    title='Número de viajes por hora durante febrero de 2021',
    labels={'Fecha_Hora': 'Fecha y Hora', 'Numero_viajes': 'Número de viajes'},
)

# Mejorar la visualización con detalles en los ejes y el hover
fig1.update_layout(
    xaxis_title='Fecha y Hora',
    yaxis_title='Número de viajes',
    template='plotly',
    hovermode='x'
)

# Mostrar el gráfico interactivo
fig1.show()

# **Gráfico 2: Promedio de viajes por hora del día**
# Calcular el número promedio de viajes por cada hora
average_trips_per_hour = df.groupby('hour').size().groupby(level=0).mean()

# Crear un gráfico de barras interactivo con Plotly
fig2 = px.bar(
    average_trips_per_hour,
    x=average_trips_per_hour.index,
    y=average_trips_per_hour.values,
    labels={'x': 'Hora del día', 'y': 'Número promedio de viajes'},
    title='Número promedio de viajes que comenzaron a cada hora del día en febrero de 2021'
)

# Configurar ejes y diseño para mejor visualización
fig2.update_layout(
    xaxis_title='Hora del día',
    yaxis_title='Número promedio de viajes',
    xaxis=dict(dtick=1),  # Mostrar todas las horas en el eje X
    template='plotly',     # Estilo para hacer el gráfico más agradable
    hovermode="x"          # Para un efecto de hover intuitivo
)

# Mostrar el gráfico interactivo
fig2.show()

# **Gráfico 3: Promedio de viajes por día de la semana**
# Extraer el día de la semana como nombre (lunes, martes, etc.)
df['weekday'] = df['tsO'].dt.day_name()

# Calcular el número promedio de viajes por cada día de la semana
average_trips_per_weekday = df.groupby('weekday').size() / 4  # Dividido entre 4 semanas

# Asegurar el orden correcto de los días
ordered_days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']
average_trips_per_weekday = average_trips_per_weekday.reindex(ordered_days)

# Crear un gráfico de barras interactivo con Plotly
fig3 = px.bar(
    average_trips_per_weekday,
    x=average_trips_per_weekday.index,
    y=average_trips_per_weekday.values,
    labels={'x': 'Día de la semana', 'y': 'Número promedio de viajes'},
    title='Número promedio de viajes que comenzaron cada día de la semana en febrero de 2021'
)

# Configurar ejes y diseño para mejor visualización
fig3.update_layout(
    xaxis_title='Día de la semana',
    yaxis_title='Número promedio de viajes',
    xaxis=dict(categoryorder='array', categoryarray=ordered_days),  # Mantener el orden correcto
    template='plotly',             # Estilo para hacer el gráfico más agradable
    hovermode="x"                  # Para un efecto de hover intuitivo
)

# Mostrar el gráfico interactivo
fig3.show()



# Relación entre distancia y precio
fig1 = px.scatter(
    df, x='dis', y='price', 
    title='Relación entre Distancia y Precio',
    labels={'dis': 'Distancia (m)', 'price': 'Precio (€)'},
    opacity=0.5
)
fig1.update_traces(marker=dict(size=6))
fig1.update_layout(template='plotly')

# Relación entre tiempo y precio
fig2 = px.scatter(
    df, x='tt', y='price', 
    title='Relación entre Tiempo y Precio',
    labels={'tt': 'Tiempo de Recorrido (s)', 'price': 'Precio (€)'},
    color_discrete_sequence=['orange'],
    opacity=0.5
)
fig2.update_traces(marker=dict(size=6))
fig2.update_layout(template='plotly')

# Relación entre distancia y tiempo
fig3 = px.scatter(
    df, x='dis', y='tt', 
    title='Relación entre Distancia y Tiempo de Recorrido',
    labels={'dis': 'Distancia (m)', 'tt': 'Tiempo (s)'},
    color_discrete_sequence=['purple'],
    opacity=0.5
)
fig3.update_traces(marker=dict(size=6))
fig3.update_layout(template='plotly')

# Mostrar los gráficos
fig1.show()
fig2.show()
fig3.show()





lower_percentile = df['tt'].quantile(0.01)
upper_percentile = df['tt'].quantile(0.99)
df = df[(df['tt'] >= lower_percentile) & (df['tt'] <= upper_percentile)]
print(f"Valores atípicos eliminados: {25186 - len(df)} filas")


# Relación entre distancia y precio
fig1 = px.scatter(
    df, x='dis', y='price', 
    title='Relación entre Distancia y Precio',
    labels={'dis': 'Distancia (m)', 'price': 'Precio (€)'},
    opacity=0.5
)
fig1.update_traces(marker=dict(size=6))
fig1.update_layout(template='plotly')

# Relación entre tiempo y precio
fig2 = px.scatter(
    df, x='tt', y='price', 
    title='Relación entre Tiempo y Precio',
    labels={'tt': 'Tiempo de Recorrido (s)', 'price': 'Precio (€)'},
    color_discrete_sequence=['orange'],
    opacity=0.5
)
fig2.update_traces(marker=dict(size=6))
fig2.update_layout(template='plotly')

# Relación entre distancia y tiempo
fig3 = px.scatter(
    df, x='dis', y='tt', 
    title='Relación entre Distancia y Tiempo de Recorrido',
    labels={'dis': 'Distancia (m)', 'tt': 'Tiempo (s)'},
    color_discrete_sequence=['purple'],
    opacity=0.5
)
fig3.update_traces(marker=dict(size=6))
fig3.update_layout(template='plotly')

# Mostrar los gráficos
fig1.show()
fig2.show()
fig3.show()





# Convertir la columna `tsO` a formato datetime
df['tsO'] = pd.to_datetime(df['tsO'], format='%d/%m/%Y %H:%M:%S')

# Extraer la hora de `tsO` para agrupar los datos
df['hour'] = df['tsO'].dt.hour

# Crear un mapa base centrado en la ubicación media de los datos
mapa_base = folium.Map(location=[df['latO'].mean(), df['lonO'].mean()], zoom_start=12)

# Iterar sobre cada hora y agregar un HeatMap a la capa de horas
for hour in range(24):
    # Filtrar el DataFrame para la hora actual
    df_hour = df[df['hour'] == hour]
    
    # Crear una lista de puntos (latitud, longitud) para el HeatMap
    heat_data = list(zip(df_hour['latO'], df_hour['lonO']))
    
    # Agregar un mapa de calor para esta hora
    HeatMap(heat_data, radius=10, blur=15, max_zoom=1, name=f"Hora {hour}:00").add_to(mapa_base)

# Añadir una capa de control para activar/desactivar las capas de cada hora
folium.LayerControl().add_to(mapa_base)

# Guardar el mapa interactivo
mapa_base.save('mapa_calor_por_hora.html')


# Convertir la columna `tsO` a formato datetime
df['tsO'] = pd.to_datetime(df['tsO'], format='%d/%m/%Y %H:%M:%S')

# Extraer la hora de `tsO` para agrupar los datos
df['hour'] = df['tsO'].dt.hour

# Crear un diccionario para almacenar los datos de cada hora
hourly_data = []
for hour in range(24):
    # Filtrar los datos para cada hora
    df_hour = df[df['hour'] == hour]
    
    # Crear una lista de puntos (latitud, longitud, intensidad) para el HeatMap
    heat_data = [[row['latO'], row['lonO'], 1] for index, row in df_hour.iterrows()]
    
    # Añadir los datos de la hora actual al diccionario
    hourly_data.append(heat_data)

# Crear el mapa base centrado en la ubicación media de los datos
mapa_base = folium.Map(location=[df['latO'].mean(), df['lonO'].mean()], zoom_start=12)

# Crear el mapa de calor con el control de tiempo usando colores cálidos
HeatMapWithTime(
    hourly_data,
    radius=10,
    gradient={0.2: 'yellow', 0.5: 'orange', 0.7: 'red', 1: 'darkred'},  # Colores cálidos
    auto_play=False,
    max_opacity=0.8
).add_to(mapa_base)

# Guardar el mapa interactivo
mapa_base.save('mapa_calor_por_hora_interactivo.html')







# Ocultar los warnings
warnings.filterwarnings("ignore")

# Suponiendo que el DataFrame 'df' ya está definido con las columnas 'latO', 'lonO', 'latD', 'lonD'
# Combinar las coordenadas de origen y destino en un solo conjunto de datos
coords = pd.DataFrame({
    'lat': pd.concat([df['latO'], df['latD']], ignore_index=True),
    'lon': pd.concat([df['lonO'], df['lonD']], ignore_index=True),
    'tipo': ['origen'] * len(df) + ['destino'] * len(df)
})

# Función para calcular la distancia de Haversine en kilómetros
def haversine_distance(coord1, coord2):
    return geodesic(coord1, coord2).kilometers

# Función recursiva para subdividir un clúster si su distancia media supera 500 metros
def subdivide_cluster(cluster_points, cluster_counter, max_distance=0.5):
    kmeans = KMeans(n_clusters=2, n_init=10, random_state=0)
    cluster_points['sub_cluster'] = kmeans.fit_predict(cluster_points[['lat', 'lon']])
    sub_centroids = kmeans.cluster_centers_

    results = []
    final_centroids = []

    for sub_cluster in range(2):
        sub_cluster_points = cluster_points[cluster_points['sub_cluster'] == sub_cluster].copy()
        sub_centroide = sub_centroids[sub_cluster]

        sub_distances = sub_cluster_points.apply(
            lambda row: haversine_distance((row['lat'], row['lon']), (sub_centroide[0], sub_centroide[1])), axis=1
        )

        sub_avg_distance = sub_distances.mean()
        if sub_avg_distance > max_distance:
            subdivided_results = subdivide_cluster(sub_cluster_points, cluster_counter, max_distance)
            for result in subdivided_results:
                final_centroids.append(result['centroid'])
                results.append(result)
        else:
            cluster_counter += 1
            final_centroids.append(sub_centroide)
            results.append({
                'centroid': sub_centroide,
                'metrics': {
                    'Num_Puntos': len(sub_cluster_points),
                    'Distancia_Media_km': sub_avg_distance
                }
            })
    return results

# Proceso principal
n_inicial = 1
min_clusters = np.inf
best_metrics = None
optimal_centroids = []

while n_inicial < min_clusters:
    print(f"Probando con {n_inicial} clústeres iniciales...")
    coords['cluster'] = KMeans(n_clusters=n_inicial, n_init=10, random_state=0).fit_predict(coords[['lat', 'lon']])

    cluster_counter = 0
    iteration_metrics = []
    iteration_centroids = []

    for cluster in range(n_inicial):
        cluster_points = coords[coords['cluster'] == cluster].copy()
        centroide = KMeans(n_clusters=1, n_init=1, random_state=0).fit(cluster_points[['lat', 'lon']]).cluster_centers_[0]

        distances = cluster_points.apply(
            lambda row: haversine_distance((row['lat'], row['lon']), (centroide[0], centroide[1])), axis=1
        )

        avg_distance = distances.mean()

        if avg_distance > 0.5:
            print(f"Subdividiendo cluster {cluster} con distancia media {avg_distance:.2f} km.")
            subdivided_results = subdivide_cluster(cluster_points, cluster_counter)
            for result in subdivided_results:
                iteration_centroids.append(result['centroid'])
                iteration_metrics.append(result['metrics'])
        else:
            cluster_counter += 1
            iteration_centroids.append(centroide)
            iteration_metrics.append({
                'Num_Puntos': len(cluster_points),
                'Distancia_Media_km': avg_distance
            })

    iteration_clusters = len(iteration_metrics)
    print(f"Número de clústeres tras iteración con {n_inicial} clústeres iniciales: {iteration_clusters}")

    if iteration_clusters <= min_clusters:  # Actualiza siempre que se encuentre un número igual o menor de clusters
        min_clusters = iteration_clusters
        best_metrics = pd.DataFrame(iteration_metrics)
        optimal_centroids = iteration_centroids.copy()  # Guarda los centroides de los clústeres óptimos

    n_inicial += 1

# Calcular porcentajes de puntos de origen y destino para cada clúster óptimo
final_results = []
for centroid_idx, centroid in enumerate(optimal_centroids):
    cluster_points = coords[coords['cluster'] == centroid_idx]
    total_points = len(cluster_points)
    origen_points = len(cluster_points[cluster_points['tipo'] == 'origen'])
    destino_points = len(cluster_points[cluster_points['tipo'] == 'destino'])

    percent_origen = (origen_points / total_points) * 100 if total_points > 0 else 0
    percent_destino = (destino_points / total_points) * 100 if total_points > 0 else 0

    # Añadir los datos del clúster a la lista final
    final_results.append({
        'lat': centroid[0],
        'lon': centroid[1],
        'Num_Puntos': best_metrics.iloc[centroid_idx]['Num_Puntos'],
        'Distancia_Media_km': best_metrics.iloc[centroid_idx]['Distancia_Media_km'],
        'Percent_Origen': percent_origen,
        'Percent_Destino': percent_destino
    })

# Crear un DataFrame con los resultados finales
final_results_df = pd.DataFrame(final_results)

# Guardar en un único archivo CSV
final_results_df.to_csv("final_clusters_results.csv", index=False)

print("Resultados de los clústeres óptimos con porcentajes guardados en 'final_clusters_results.csv'")



import folium

# Crear un mapa base centrado en el promedio de las coordenadas
map_center = [coords['lat'].mean(), coords['lon'].mean()]
map_clusters = folium.Map(location=map_center, zoom_start=12)

# Color único para todos los puntos
point_color = "#3186cc"  # Azul estándar para los puntos

# Añadir puntos de cada clúster al mapa (todos del mismo color)
for _, row in coords.iterrows():
    folium.CircleMarker(
        location=(row['lat'], row['lon']),
        radius=3,
        color=point_color,
        fill=True,
        fill_color=point_color,
        fill_opacity=0.6
    ).add_to(map_clusters)

# Añadir los centroides al mapa
for centroid_idx, centroid in enumerate(optimal_centroids):
    folium.Marker(
        location=(centroid[0], centroid[1]),
        icon=folium.Icon(color='red', icon='info-sign'),
        popup=f"Cluster {centroid_idx}"
    ).add_to(map_clusters)

# Guardar el mapa como un archivo HTML
map_clusters.save("clusters_map.html")

print("Mapa geográfico con clústeres guardado en 'clusters_map.html'")



import warnings
import pandas as pd
from sklearn.cluster import KMeans
from geopy.distance import geodesic
import numpy as np

# Ocultar los warnings
warnings.filterwarnings("ignore")

# Crear una copia del DataFrame original
df_actualizado = df.copy()

# Combinar las coordenadas de origen y destino en un solo conjunto de datos
coords = pd.DataFrame({
    'lat': pd.concat([df['latO'], df['latD']], ignore_index=True),
    'lon': pd.concat([df['lonO'], df['lonD']], ignore_index=True),
    'tipo': ['origen'] * len(df) + ['destino'] * len(df)
})

# Función para calcular la distancia de Haversine en kilómetros
def haversine_distance(coord1, coord2):
    return geodesic(coord1, coord2).kilometers

# Función recursiva para subdividir un clúster si su distancia media supera 500 metros
def subdivide_cluster(cluster_points, cluster_counter, max_distance=0.5):
    kmeans = KMeans(n_clusters=2, n_init=10, random_state=0)
    cluster_points['sub_cluster'] = kmeans.fit_predict(cluster_points[['lat', 'lon']])
    sub_centroids = kmeans.cluster_centers_

    results = []
    final_centroids = []

    for sub_cluster in range(2):
        sub_cluster_points = cluster_points[cluster_points['sub_cluster'] == sub_cluster].copy()
        sub_centroide = sub_centroids[sub_cluster]

        sub_distances = sub_cluster_points.apply(
            lambda row: haversine_distance((row['lat'], row['lon']), (sub_centroide[0], sub_centroide[1])), axis=1
        )

        sub_avg_distance = sub_distances.mean()
        if sub_avg_distance > max_distance:
            subdivided_results = subdivide_cluster(sub_cluster_points, cluster_counter, max_distance)
            for result in subdivided_results:
                final_centroids.append(result['centroid'])
                results.append(result)
        else:
            cluster_counter += 1
            final_centroids.append(sub_centroide)
            results.append({
                'centroid': sub_centroide,
                'metrics': {
                    'Num_Puntos': len(sub_cluster_points),
                    'Distancia_Media_km': sub_avg_distance
                }
            })
    return results

# Proceso principal
n_inicial = 1
min_clusters = np.inf
best_metrics = None
optimal_centroids = []

while n_inicial < min_clusters:
    print(f"Probando con {n_inicial} clústeres iniciales...")
    coords['cluster'] = KMeans(n_clusters=n_inicial, n_init=10, random_state=0).fit_predict(coords[['lat', 'lon']])

    cluster_counter = 0
    iteration_metrics = []
    iteration_centroids = []

    for cluster in range(n_inicial):
        cluster_points = coords[coords['cluster'] == cluster].copy()
        centroide = KMeans(n_clusters=1, n_init=1, random_state=0).fit(cluster_points[['lat', 'lon']]).cluster_centers_[0]

        distances = cluster_points.apply(
            lambda row: haversine_distance((row['lat'], row['lon']), (centroide[0], centroide[1])), axis=1
        )

        avg_distance = distances.mean()

        if avg_distance > 0.5:
            print(f"Subdividiendo cluster {cluster} con distancia media {avg_distance:.2f} km.")
            subdivided_results = subdivide_cluster(cluster_points, cluster_counter)
            for result in subdivided_results:
                iteration_centroids.append(result['centroid'])
                iteration_metrics.append(result['metrics'])
        else:
            cluster_counter += 1
            iteration_centroids.append(centroide)
            iteration_metrics.append({
                'Num_Puntos': len(cluster_points),
                'Distancia_Media_km': avg_distance
            })

    iteration_clusters = len(iteration_metrics)
    print(f"Número de clústeres tras iteración con {n_inicial} clústeres iniciales: {iteration_clusters}")

    if iteration_clusters <= min_clusters:  # Actualiza siempre que se encuentre un número igual o menor de clusters
        min_clusters = iteration_clusters
        best_metrics = pd.DataFrame(iteration_metrics)
        optimal_centroids = iteration_centroids.copy()  # Guarda los centroides de los clústeres óptimos

    n_inicial += 1

# Determinar los clústeres finales para los puntos de origen y destino
# Aplicar el modelo KMeans final para asignar los puntos a los clústeres óptimos
final_kmeans = KMeans(n_clusters=len(optimal_centroids), init=np.array(optimal_centroids), n_init=1, random_state=0)
coords['final_cluster'] = final_kmeans.fit_predict(coords[['lat', 'lon']])

# Asignar los clústeres al DataFrame actualizado
df_actualizado['cluster_idO'] = final_kmeans.predict(
    df[['latO', 'lonO']].rename(columns={'latO': 'lat', 'lonO': 'lon'})
)
df_actualizado['cluster_idD'] = final_kmeans.predict(
    df[['latD', 'lonD']].rename(columns={'latD': 'lat', 'lonD': 'lon'})
)

# Calcular métricas finales para cada clúster
metrics = []
for cluster_id in range(len(optimal_centroids)):
    cluster_points = coords[coords['final_cluster'] == cluster_id]
    num_puntos = len(cluster_points)
    centroid = final_kmeans.cluster_centers_[cluster_id]
    distances = cluster_points.apply(
        lambda row: haversine_distance((row['lat'], row['lon']), (centroid[0], centroid[1])),
        axis=1
    )
    distancia_media = distances.mean()
    metrics.append({
        'Cluster_ID': cluster_id,
        'Num_Puntos': num_puntos,
        'Distancia_Media_km': distancia_media,
        'Centroid_lat': centroid[0],
        'Centroid_lon': centroid[1]
    })

# Crear un DataFrame con las métricas
metrics_df = pd.DataFrame(metrics)

# Guardar los resultados
df_actualizado.to_csv("df_actualizado_con_clusters.csv", index=False)
metrics_df.to_csv("metrics_clusters_finales.csv", index=False)

print("El DataFrame actualizado con los clústeres se ha guardado en 'df_actualizado_con_clusters.csv'")
print("Las métricas de los clústeres se han guardado en 'metrics_clusters_finales.csv'")



import folium
import pandas as pd
from ipywidgets import interact, widgets
import matplotlib.pyplot as plt
from datetime import datetime

# Cargar el DataFrame actualizado
df_actualizado = pd.read_csv("df_actualizado_con_clusters.csv")

# Convertir las columnas de timestamp a formato datetime
df_actualizado['tsO'] = pd.to_datetime(df_actualizado['tsO'], format="%d/%m/%Y %H:%M:%S")
df_actualizado['tsD'] = pd.to_datetime(df_actualizado['tsD'], format="%d/%m/%Y %H:%M:%S")

# Generar una lista de colores únicos para los 40 clústeres
colors = plt.cm.get_cmap('tab20b', 40)  # Usa una paleta con 40 colores distintos
cluster_colors = {i: f"#{int(colors(i)[0]*255):02x}{int(colors(i)[1]*255):02x}{int(colors(i)[2]*255):02x}" for i in range(40)}

# Función para crear el mapa interactivo
def crear_mapa(dia='Todo el mes'):
    # Filtrar los datos por el día seleccionado
    if dia != 'Todo el mes':
        fecha_filtrada = datetime.strptime(dia, "%d/%m/%Y")
        df_filtrado = df_actualizado[
            (df_actualizado['tsO'].dt.date == fecha_filtrada.date()) |
            (df_actualizado['tsD'].dt.date == fecha_filtrada.date())
        ]
    else:
        df_filtrado = df_actualizado

    # Crear el mapa base centrado en el promedio de las coordenadas
    map_center = [df_filtrado['latO'].mean(), df_filtrado['lonO'].mean()]
    mapa = folium.Map(location=map_center, zoom_start=12)

    # Añadir puntos de origen y destino al mapa
    for _, row in df_filtrado.iterrows():
        # Puntos de origen
        folium.CircleMarker(
            location=(row['latO'], row['lonO']),
            radius=3,
            color=cluster_colors[row['cluster_idO']],
            fill=True,
            fill_color=cluster_colors[row['cluster_idO']],
            fill_opacity=0.6,
            tooltip=f"Origen - Cluster {row['cluster_idO']}, {row['tsO']}"
        ).add_to(mapa)

        # Puntos de destino
        folium.CircleMarker(
            location=(row['latD'], row['lonD']),
            radius=3,
            color=cluster_colors[row['cluster_idD']],
            fill=True,
            fill_color=cluster_colors[row['cluster_idD']],
            fill_opacity=0.6,
            tooltip=f"Destino - Cluster {row['cluster_idD']}, {row['tsD']}"
        ).add_to(mapa)

    # Añadir los centroides al mapa
    for cluster_id in range(40):
        cluster_points = df_actualizado[
            (df_actualizado['cluster_idO'] == cluster_id) |
            (df_actualizado['cluster_idD'] == cluster_id)
        ]
        centroid_lat = cluster_points[['latO', 'latD']].mean().mean()
        centroid_lon = cluster_points[['lonO', 'lonD']].mean().mean()

        folium.Marker(
            location=(centroid_lat, centroid_lon),
            icon=folium.Icon(color='red', icon='info-sign'),
            popup=f"Cluster {cluster_id}"
        ).add_to(mapa)

    return mapa

# Generar lista de días únicos
dias_unicos = df_actualizado['tsO'].dt.date.unique()
dias_unicos = [fecha.strftime("%d/%m/%Y") for fecha in dias_unicos]
dias_menu = ['Todo el mes'] + dias_unicos

# Crear el menú interactivo
@interact
def mostrar_mapa(dia=dias_menu):
    mapa = crear_mapa(dia)
    display(mapa)






# Cargar el archivo XLSX en un DataFrame llamado energia_df
energia_df = pd.read_excel('20210201_20210228_PUN.xlsx') 
energia_df, energia_df.info()


# Asegurarse de que todos los valores en la columna `€/MWh` sean cadenas antes de aplicar operaciones de cadena
energia_df['€/MWh'] = energia_df['€/MWh'].astype(str).str.replace(',', '.')

# Convertir la columna `€/MWh` a numérica
energia_df['€/MWh'] = pd.to_numeric(energia_df['€/MWh'], errors='coerce')

# Convertir la columna `Data` a formato datetime con el formato adecuado
energia_df['Data'] = pd.to_datetime(energia_df['Data'], format='%d/%m/%Y')

# Crear una columna de fecha y hora combinando `Data` y `Ora`
energia_df['Fecha_Hora'] = energia_df['Data'] + pd.to_timedelta(energia_df['Ora'], unit='h')

# Crear el gráfico interactivo
fig = px.line(
    energia_df,
    x='Fecha_Hora',
    y='€/MWh',
    title='Evolución del coste de la energía en Roma durante febrero de 2021',
    labels={'Fecha_Hora': 'Fecha y Hora', '€/MWh': 'Coste en €/MWh'}
)

# Mejorar la visualización con detalles en los ejes y el hover
fig.update_layout(
    xaxis_title='Fecha y Hora',
    yaxis_title='Coste en €/MWh',
    template='plotly',
    hovermode='x'
)

# Mostrar el gráfico interactivo
fig.show()



# Paso 1: Asegurarse de que la columna '€/MWh' sea numérica
energia_df['€/MWh'] = pd.to_numeric(energia_df['€/MWh'], errors='coerce')

# Paso 2: Agrupar los datos por hora y calcular el precio promedio por hora
precio_promedio_por_hora = energia_df.groupby('Ora')['€/MWh'].mean()

# Paso 3: Visualizar los resultados
plt.figure(figsize=(10,6))
plt.plot(precio_promedio_por_hora.index, precio_promedio_por_hora.values, marker='o', color='b')
plt.title('Variación del precio promedio de la electricidad por hora en febrero de 2021')
plt.xlabel('Hora del día')
plt.ylabel('Precio promedio (€ / MWh)')
plt.xticks(range(1, 25))  # Aseguramos que las horas vayan de 1 a 24
plt.grid(True)

# Paso 4: Identificar las horas con los precios más altos y más bajos
hora_max_precio = precio_promedio_por_hora.idxmax()
hora_min_precio = precio_promedio_por_hora.idxmin()
max_precio = precio_promedio_por_hora.max()
min_precio = precio_promedio_por_hora.min()

# Mostrar las horas de los precios más altos y más bajos
print(f"Hora con el precio más alto: {hora_max_precio} h, Precio: €{max_precio:.2f}")
print(f"Hora con el precio más bajo: {hora_min_precio} h, Precio: €{min_precio:.2f}")

# Paso 5: Mostrar el gráfico
plt.show()



